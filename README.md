# AI-Safety-Project

This project represents my culminating work for the [AI Safety Fundamentals: AI Alignment Course](https://aisafetyfundamentals.com/alignment/), completed in September 2024. More details are available on the [Github Page here](https://nidone.github.io/AI-Safety-Project/). 

## LLM Safety for Youth

The focus of this project is on evaluating the safety of large language models (LLMs) concerning their interactions with youth—an area that, while crucial, has not been extensively researched. This topic was selected not only for its importance but also to apply and deepen my understanding of the technical methodologies learned during the course, particularly in AI evaluation practices.

## Feedback Welcome!

I greatly appreciate feedback on any aspect of this project—from typos to potential misunderstandings of the approaches used. Your insights will help improve the quality and accuracy of this work.