---
layout: default
title: Home - Evaluating LLM Safety for Youth
---

# AI Safety Course Project

This project represents my final contribution to [the AI Safety Fundamentals: AI Alignment Course](https://aisafetyfundamentals.com/alignment/), held in 2024.

In this initiative, I aim to assess the safety of large language models (LLMs), specifically their potential adverse impacts on the emotional and cognitive development of youth. These impacts include diminished real-world interactions, negative effects on learning, exposure to inappropriate content, and the provision of misleading advice.

You can view a quick summary in [this 3-minute video](https://www.youtube.com/watch?v=bJ-xAjvkxzw) or find more details on my [GitHub repository](https://github.com/nidone/AI-Safety-Project).

## Background

Over recent decades, rapid technological advancements have unintentionally affected children and adolescents. Notable instances include:

* Google and YouTube, fined $170 million for breaching children's privacy laws ([source](https://www.ftc.gov/news-events/news/press-releases/2019/09/google-youtube-will-pay-record-170-million-alleged-violations-childrens-privacy-law))
* The book [Anxious Generation](https://www.anxiousgeneration.com/book), which discusses how smartphones and social networks, particularly Instagram, have potentially caused irreversible harm to adolescents.
* Concerns raised by Oxford researchers that AI ethics often neglect children's issues ([source](https://www.ox.ac.uk/news/2024-03-21-ai-ethics-are-ignoring-children-say-oxford-researchers)) and by UK Children's Commissioner ([source](https://www.childrenscommissioner.gov.uk/blog/the-childrens-commissioners-view-on-artificial-intelligence-ai/)).

As advanced AI systems rapidly integrate into daily life, much of the AI Safety community’s attention has focused on AI-specific risks such as bias, disinformation, and fairness. However, when new technologies like YouTube, Instagram, and smartphones became widely adopted at an exponential pace, the most vulnerable populations—particularly youth—were profoundly impacted, often without adequate safeguards.

Despite these precedents, research on how LLMs specifically affect youth development remains limited. As AI labs continue to compete and push for greater market share, the risk to children and adolescents may increase. I think this issue could potentially be one of the significant "Unknown Risks" that AI safety must address more proactively.

## Project Goals
1. Understand the current state of LLM Safety for Youth.
2. Personally engage in building an evaluation dataset to deepen my understanding, as evaluations are a crucial aspect of AI alignment, as learned throughout the course.
3. Share my methodology and findings publicly to:
  * Receive feedback on my approach, thought process, dataset, and evaluation results.
  * Encourage others to create evaluation datasets for themes important to them.

## Get Started

To begin exploring this project, you can jump-start with the [Methodology section](https://nidone.github.io/AI-Safety-Project/methodology), which details the approaches employed in this study.
<br /> <br />

