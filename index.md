---
layout: default
title: Home - Evaluating LLM Safety for Youth
---

# Welcome to my AI Safety Course Project

This project is part of my final work for [the AI Safety Fundamentals: AI Alignment Course](https://aisafetyfundamentals.com/alignment/).

In this project, I aim to evaluate the safety of large language models (LLMs), focusing specifically on their potential negative impacts on the emotional and cognitive development of youth, such as reduced real-world connection, negative impacts on learning, exposure to inappropriate content, and misleading advice.

You can find more information about this project at [my GitHub repository](https://github.com/nidone/AI-Safety-Project).(currently private).

## Background

Over the past few decades, we’ve witnessed rapid technological advancements, some of which have unintentionally affected children and adolescents. A few notable examples include:

* Google and YouTube were [fined $170 million](https://www.ftc.gov/news-events/news/press-releases/2019/09/google-youtube-will-pay-record-170-million-alleged-violations-childrens-privacy-law) for violating children's privacy laws.
* The book [Anxious Generation](https://www.anxiousgeneration.com/book) highlights how smartphones and social networking services, especially Instagram, may have caused irreversible harm to adolescents.
* Oxford researchers have [raised concerns](https://www.ox.ac.uk/news/2024-03-21-ai-ethics-are-ignoring-children-say-oxford-researchers) that AI ethics discussions often overlook the impact on children.

Much of the research on AI safety to date has focused on issues like bias, disinformation, and fairness. However, there appears to be fairly limited research on the effects of LLMs on youth development. As the competition among AI labs intensifies, I thought there could be a growing risk that the most vulnerable demographic—children and adolescents—may once again be negatively impacted.

## Project Goals
1. Raise awareness about AI's potential risk on youth
2. Going through the process of buidling an eval dataset myself and deeply learn, as evals seems a **very** important part of AI alignment we have learned through the cource
3. Share the process and data externally, so that
  * I can have feedback on methodology, thorughprocess, dataset and eval results
  * More people can build eval datasets for the various themes that matter to them.

## Get Started

To begin exploring this project, start with the [Methodology section](https://nidone.github.io/AI-Safety-Project/methodology), which outlines the approach used in this project.
<br /> <br />

