---
layout: default
title: Home - Evaluating LLM Safety for Youth
---

# AI Safety Course Project

This project represents my final contribution to [the AI Safety Fundamentals: AI Alignment Course](https://aisafetyfundamentals.com/alignment/).

In this initiative, I aim to assess the safety of large language models (LLMs), specifically their potential adverse impacts on the emotional and cognitive development of youth. These impacts include diminished real-world interactions, negative effects on learning, exposure to inappropriate content, and the provision of misleading advice.

For more details about this project, visit my [GitHub repository](https://github.com/nidone/AI-Safety-Project).

## Background

Over recent decades, rapid technological advancements have unintentionally affected children and adolescents. Notable instances include:

* Google and YouTube, fined $170 million for breaching children's privacy laws ([source](https://www.ftc.gov/news-events/news/press-releases/2019/09/google-youtube-will-pay-record-170-million-alleged-violations-childrens-privacy-law))
* The book [Anxious Generation](https://www.anxiousgeneration.com/book), which discusses how smartphones and social networks, particularly Instagram, have potentially caused irreversible harm to adolescents.
* Concerns raised by Oxford researchers that AI ethics often neglect children's issues ([source](https://www.ox.ac.uk/news/2024-03-21-ai-ethics-are-ignoring-children-say-oxford-researchers))

While much research on AI safety has focused on bias, disinformation, and fairness, research into the effects of LLMs on youth development remains limited. As AI labs continue to compete, the risk to the most vulnerable group—children and adolescents—may increase.

## Project Goals
1. Understand the current state of LLM Safety for Youth.
2. Personally engage in building an evaluation dataset to deepen my understanding, as evaluations are a crucial aspect of AI alignment, as learned throughout the course.
3. Share my methodology and findings publicly to:
  * Receive feedback on my approach, thought process, dataset, and evaluation results.
  * Encourage others to create evaluation datasets for themes important to them.

## Get Started

To begin exploring this project, you can jump-start with the [Methodology section](https://nidone.github.io/AI-Safety-Project/methodology), which details the approaches employed in this study.
<br /> <br />

