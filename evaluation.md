---
layout: default
title: Evaluation - AI Safety Project
---

# Evaluation

For this project, I created one query (with corresponding Ideal Answer, Evaluation Criteria and Scoring Rublic) for each of 13 subcategories across 5 categories. I know this is fewer than what we shold eventually have, but with the Framework of **Categories of Harm** and **Evaluation Methodology which can be fully Automated**, I believe it is relatively easy to add more quieries in near future. 

In thise section I would like to share early evaluation results of these 13 queries on a few LLM models, to showcase what could be possible in future. 

## Evaluated Models

I picked widely used LLM models from OpenAI, Google and Anthropic. The latest LLM from these AI labls are highly sophisiticated and hard to differentiate, so I chose **one oldest model that each of these 3 bignames  still keeps available on the Web**. Namely: 
  - OpenAI’s ChatGPT (GPT-4)
  - Google's Gemini (Advanced and Normal)
  - Anthropic’s Claude (3.0 Haiku)

- I also picked up **2 much less sophsticated model I can access via API** for comparison purpose:
  - [OpenAI API](https://colab.research.google.com/drive/16R7Kv-IFijBwdka3WOE2Gs0g0P3iyrD0): GPT-3.5
  - [Hugging Face](https://colab.research.google.com/drive/15AcYFMU5p8khYKFzSPVZ47FlGq8h0gUD): GPT-Neo
  - ELYZA 70B on AWS Bedrock

## Results

Here is [WIP results](https://docs.google.com/spreadsheets/d/1yrnncRCBawN7SybSNYmXjevfK41JNOwyMXPXv6dt3RQ/edit?gid=0#gid=0)

Will add rader chart of 5 models. 

<br /> <br />

[Content remains the same as in the previous artifact]
