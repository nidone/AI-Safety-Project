---
layout: default
title: Conclusions - AI Safety Project
---

# Conclusions

- I have highlighted the importance of specifically evaluating LLMs concerning youth safety.
- Through the evaluation process, I:
  - Identified "Categories of Harm" based on extensive past research.
  - Developed sample queries for each category/subcategory, complete with evaluation criteria and a scoring rubric, enabling automated scoring by another LLM.
  - Conducted evaluations using a small dataset across five popular LLMs.
- Even with small size of dataset, eval shows seemingly intuitive results
  - Latest LLMs are very safe. Their improvement in the past few years is very solid. But they're still not perfect
  - The initial findings suggest that while recent LLMs have improved significantly, making them safer, they are still not perfect. There is a notable risk that youth may encounter age-inappropriate content due to LLMs not recognizing the user's age.

## Discussions

- **Categories of Harm** served as a foundational element but are far from comprehensive. Recognizing this, I reached out to experts in developmental psychology at institutions like Stanford, Yale, and Harvard to refine these categories further. Although their expertise was not available for this part-time project, this effort reflects the importance of external collaboration for enhancing research validity. 
- **Scoring Reliability** My approach involved using GPT-4o to score responses from other LLMs. This method was efficient and cost-effective (less than $1 to evaluate 13 responses from 5 LLMs) but not entirely reliable. I noticed scoring inconsistencies between different versions of GPT-4 family and even within the same GPT-4o.

## Future Work

- **Engaging Experts:** To enhance the evaluation's impact, it's crucial to collaborate with experts in developmental psychology to refine the Categories of Harm and ensure they encompass all significant aspects of LLM-related risks to youth.
- **Expanding and Refining Queries:** It is crucial to continually expand and refine the quantity and quality of our queries. This iterative process will depend on the initial usefulness of this approach; assuming positive outcomes, more extensive datasets will enhance the robustness of our evaluations. Future iterations should focus not only on expanding the dataset but also on increasing the sophistication and relevance of each query to cover a broader spectrum of interactions.
- **Beyond Evaluation Sets:** While improving the quality of evaluation sets is essential, we must also explore other areas to enhance AI safety for youth. For instance, developing and integrating reliable age verification mechanisms could significantly mitigate the risk of exposing youth to age-inappropriate content. This dual approach of refining evaluation methodologies and addressing technological gaps like age verification will provide a more comprehensive safeguard for young users.
- **Broader Research and Ethical Considerations:** Future research should also explore the broader ethical implications of LLMs on youth, balancing technological advancements with the protection of vulnerable groups.

<br /> <br />

[Content remains the same as in the previous artifact]
